# Needed Camus properties, more cleanup to come

# source top-level data output directory, sub-directory will be scanned for topics
camus.sweeper.source.dir=

# final top-level data output directory, sub-directory will be dynamically created for each topic compacted
# if this property is empty, the source dir will be used as the destination dir.
camus.sweeper.dest.dir=

# subdirectory found within a topic directory, these directories will contain the actual partitions
camus.sweeper.source.subdir=
camus.sweeper.dest.subdir=

# if whitelist has values, only whitelisted topic are compacted.  nothing on the blacklist is compacted
camus.sweeper.whitelist=
camus.sweeper.blacklist=

# configure output compression
mapred.output.compress=true
mapred.output.compression.codec=deflate
avro.mapred.deflate.level=6

camus.default.timezone=America/Los_Angeles

mapred.max.split.size=471859200
mapred.min.split.size=471859200

# max number of files for a single partition
max.files=30

# this can be set to any class extending com.linkedin.camus.sweeper.CamusSweeperPlanner
# this class should know how the data is and should be partitioned
camus.sweeper.planner.class=com.linkedin.camus.sweeper.CamusSweeperDatePartitionPlanner

# this can be set to any class extending com.linkedin.camus.sweeper.mapreduce.CamusSweeperJob
# this utility class is used to setup the parameters for data input and output
camus.sweeper.io.configurer.class=com.linkedin.camus.sweeper.mapreduce.CamusSweeperAvroKeyJob

# if defined, this schema will be used to dedup the events from the source partitions.
# this schema should be a subset of the input avro schema and only contain fields present and 
# required by the source schema. For example:
#camus.sweeper.avro.key.schema={ "name": "GenericEventWithGuid", "type": "record", "fields": [ { "name": "guid", "type": "string" } ] }
camus.sweeper.avro.key.schema=