# Needed Camus properties, more cleanup to come

# final top-level data output directory, sub-directory will be dynamically created for each topic pulled
etl.destination.path=
# HDFS location where you want to keep execution files, i.e. offsets, error logs, and count files
etl.execution.base.path=
# where completed Camus job output directories are kept, usually a sub-dir in the base.path
etl.execution.history.path=

zookeeper.hosts=
zookeeper.broker.topics=/brokers/topics
zookeeper.broker.nodes=/brokers/ids

etl.schema.registry.url=
etl.kafka.schemaregistry.client.class=com.linkedin.batch.etl.kafka.schemaregistry.AvroJdbcSchemaRegistryClient

# all files in this dir will be added to the distributed cache and placed on the classpath for hadoop tasks
# hdfs.default.classpath.dir=

# max hadoop tasks to use, each task can pull multiple topic partitions
mapred.map.tasks=30
# max historical time that will be pulled from each partition based on event timestamp
kafka.max.pull.hrs=1
# events with a timestamp older than this will be discarded. 
kafka.max.historical.days=3
# max bytes pull for a topic-partition in a single run
kafka.max.pull.megabytes.per.topic=4096

# if whitelist has values, only whitelisted topic are pulled.  nothing on the blacklist is pulled
kafka.blacklist.topics=
kafka.whitelist.topics=

# everything below this point can be ignored for the time being, will provide more documentation down the road
##########################
etl.run.tracking.post=false
kafka.monitor.tier=
etl.counts.path=
kafka.monitor.time.granularity=10

etl.hourly=hourly
etl.daily=daily
etl.ignore.schema.errors=false

etl.default.timezone=America/Los_Angeles
etl.deflate.level=6
etl.output.file.time.partition.mins=60
etl.keep.count.files=false
etl.execution.history.max.of.quota=.8

mapred.output.compress=true
mapred.map.max.attempts=1

kafka.client.buffer.size=20971520
kafka.client.so.timeout=60000

#zookeeper.session.timeout=
#zookeeper.connection.timeout=